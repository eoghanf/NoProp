# MNIST NoProp Training Configuration

# Dataset settings
dataset: "mnist"
batch_size: 128
data_path: "./data"
num_workers: 4

# Model architecture  
num_layers: 10
hidden_dim: 256

# Training parameters
epochs: 100
learning_rate: 0.001
weight_decay: 0.001

# NoProp specific parameters
timesteps: 10  # Same as num_layers typically
eta: 0.1       # Î· hyperparameter from paper (currently unused)

# Optimization settings
grad_clip_max_norm: 1.0
embed_lr_multiplier: 0.1  # Lower learning rate for embedding matrix

# Logging and saving
log_interval: 100
save_best: true
save_final: true
best_model_path: "best_mnist_noprop_model.pt"
final_model_path: "final_mnist_noprop_model.pt"

# Reproducibility
seed: 42

# Early stopping (optional)
early_stopping: false
early_stopping_accuracy: 99.5